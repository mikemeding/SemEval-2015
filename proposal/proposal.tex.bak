\documentclass[dvips,12pt]{article}

% Any percent sign marks a comment to the end of the line

% Every latex document starts with a documentclass declaration like this
% The option dvips allows for graphics, 12pt is the font size, and article
%   is the style

\usepackage[pdftex]{graphicx}
\usepackage{url}

% These are additional packages for "pdflatex", graphics, and to include
% hyperlinks inside a document.

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

% These force using more of the margins that is the default style

\begin{document}

% Everything after this becomes content
% Replace the text between curly brackets with your own

\title{SemEval 2015 Task 1}
\author{Mike Meding \& Hoanh Nguyen}
\date{\today}

% You can leave out "date" and it will be added automatically for today
% You can change the "\today" date to any text you like


\maketitle

%By the time you submit your project proposal, you should know what system you will be implementing as your baseline.
%
%A project proposal should contain:
%− A statement of the problem: problem definition which clearly specifies input and output.
%− A review of the relevant literature; identify the state-of-the-art approach you will implement as your baseline.
%− An outline of the approach you plan to take: 
%   (a) A description of the algorithms and approaches you plan to implement 
%   (b) Tools you are planning to use
%− An overall plan for experiments & evaluation
%− Real world data-set(s) that will be used for evaluation
%
%Project proposals are due Mon Mar 2. 
%
%Plese submit your project proposals as follows:
%
%   $ submit arum project-proposal items-to-submit


\section{Introduction}

%− A statement of the problem: problem definition which clearly specifies input and output.
\paragraph{}
Our choice for the project this semester is SemEval 2015 Task 1. This task has to do with paraphrasing tweets to divulge similarity between them. In bold below is the problem as stated directly from the SemEval website.
\textbf{Given two sentences, the participants are asked to determine whether they express the same or very similar meaning and optionally a degree score between 0 and 1.}


\section{Relevant Text}
%− A review of the relevant literature; identify the state-of-the-art approach you will implement as your baseline.
\begin{itemize}
\item Andrew, Goldberg (2007). Automatic Summarization
\item Hercules, Dalianis (2003). Porting and Evaluation of Automatic Summarization
\item Wei Xu and Alan Ritter and Chris Callison-Burch and William B. Dolan and Yangfeng Ji (2014). Extracting Lexically Divergent Paraphrases from Twitter
\item \textbf{These are in addition to the related readings on the SemEval page}
\end{itemize}



\section{Approach}
%− An outline of the approach you plan to take: 
%   (a) A description of the algorithms and approaches you plan to implement 
\paragraph{}
The great part about doing a project from SemEval is that all baseline models and data are given. For our baseline we have the option of choosing between two models. One is a logistic regression model with an f-score of 0.6 using the included test and training data, the other model is a weighted factorization matrix with similar results. Both of these baselines are written in Python and for the purposes of our implementation may be rewritten to Java. This remains undecided as of this writing.

\paragraph{}
The tools we plan to use are the standard NLP state-of-the-art toolkits such as POS taggers and sentiment analysers. All of which are well documented for use in both Java and Python.


\section{Experiments \& Evaluation}
%− An overall plan for experiments & evaluation
Given these baseline models we will likely improve them using semantic analysis tools such as word2vec. We may also experiment with sentiment scores to see how it effects our results. The evaluation of our results has also been standardized for this task. Included with this task is an output evaluator which will grade our results giving us an f-score thus eliminating the need for complicated evaluation methods. This method was used for the competition and is the accepted standard to which we can reflect upon any improvements that we make.

\section{Data}
%− Real world data-set(s) that will be used for evaluation
\paragraph{}
The data for this project has been provided for us by the SemEval team. This data includes 4727 development sentence pairs, 972 test pairs, and 13063 pairs for training our model. The pairs are completely random and are organized into sets of semantically similar tweets with their paraphrases extracted into categories. Additional information regarding the structure of the data can be seen in the Extracting Lexically Divergent Paraphrases from Twitter paper above.


%SCREW IT. NOT WORTH THE EFFORT.
%\begin{thebibliography}{99}

%\bibitem{gonzalez2012} Jonay I. Gonz\'{a}lez Hern\'{a}ndez, 
%Pilar Ruiz-Lapuente,	
%Hugo M. Tabernero,	
%David Montes,	
%Ramon Canal,	
%Javier M\'{e}ndez	
%and Luigi R. Bedin,
%{No surviving evolved companions of the progenitor of SN1006},
%Nature, {\bf 489}, 533-536 (2012).

%\end{thebibliography}



\end{document}
